{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライン情報と選手の脚質を取得する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取得元: WinTicket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# racecard_url = \"https://www.winticket.jp/keirin/keiokaku/racecard/2021010127/2/3\"\n",
    "# raceresult_url = \"https://www.winticket.jp/keirin/yokkaichi/raceresult/2022010448/3/8\"\n",
    "# req = requests.get(raceresult_url)\n",
    "# soup = BeautifulSoup(req.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 日程を与えるとその日開催されたレースのURLをリストで返す関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(year, month, day):\n",
    "    url = \"https://www.winticket.jp/keirin/racecard/\" + str(year) + str(month).zfill(2) + str(day).zfill(2)\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    url_soup = soup.find_all('a', rel='nofollow')\n",
    "    urls = [i.get('href') for i in url_soup]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### urlを与えると脚質のデータフレームを返す関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_legtype_table(url):\n",
    "    race_tables = pd.read_html(url)\n",
    "    legtype_table = race_tables[1][['車', '脚']].rename(columns={'車':'no', '脚':'leg'}).droplevel(1, axis=1)\n",
    "    return legtype_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### htmlを与えるとライン情報のデータフレームを返す関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_line_table(html, number_of_people):\n",
    "    line_soup = html.find_all(\"div\", class_=\"sc-1y958x7-0\")\n",
    "    line_text = [i.text for i in line_soup][0]\n",
    "    if '競' in line_text:\n",
    "        return None\n",
    "    lines = line_text.split('区切り')\n",
    "\n",
    "    line_data = [[0 for j in range(number_of_people)] for i in range(3)]\n",
    "    for n, i in enumerate(lines):\n",
    "        people = len(i)\n",
    "        for k, j in enumerate(i):\n",
    "            index = int(j) - 1\n",
    "            line_data[0][index] = people\n",
    "            line_data[1][index] = n\n",
    "            line_data[2][index] = k\n",
    "    line_data.insert(0, [i+1 for i in range(number_of_people)])\n",
    "    df = pd.DataFrame({'no':line_data[0],\n",
    "                       'people':line_data[1],\n",
    "                       'group':line_data[2],\n",
    "                       'number':line_data[3]})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### urlを与えると着順のデータフレームを返す関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result_table(url):\n",
    "    result_tables = pd.read_html(url)\n",
    "    result_table = result_tables[0][['着','車']].rename(columns={'着':'no', '車':'result'})\n",
    "    return result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ここからメイン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import traceback\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. レース情報が乗っているページのURLを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "race_urls = []\n",
    "for year in range(2021, 2022):\n",
    "    for month in range(1, 2):\n",
    "        for day in tqdm(range(1, 2)):\n",
    "            race_urls.extend(get_urls(year, month, day))\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. URLにアクセスして各種データを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [05:16<00:00,  3.73s/it]\n"
     ]
    }
   ],
   "source": [
    "race_data_7 = pd.DataFrame()\n",
    "race_data_9 = pd.DataFrame()\n",
    "\n",
    "for url in tqdm(race_urls):\n",
    "    #レースIDの設定\n",
    "    split_url = url.split('/')\n",
    "    race_id = split_url[4] + \"/\" + split_url[5] + \"/\" + split_url[6]\n",
    "\n",
    "    if (race_id in race_data_7.index) or (race_id in race_data_9.index):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        racecard_url = \"https://www.winticket.jp\" + url\n",
    "        legtype_table = create_legtype_table(racecard_url)\n",
    "        if legtype_table.empty or ('欠' in legtype_table['no']):\n",
    "            continue\n",
    "        time.sleep(1)\n",
    "\n",
    "        number_of_people = len(legtype_table)\n",
    "\n",
    "        req = requests.get(racecard_url)\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        line_table = create_line_table(soup, number_of_people)\n",
    "        time.sleep(1)\n",
    "\n",
    "        raceresult_url = racecard_url.replace(\"racecard\", \"raceresult\")\n",
    "        result_table = create_result_table(raceresult_url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        #dfの作成，結合\n",
    "        df = pd.concat([legtype_table, line_table, result_table], axis=1)\n",
    "        df = df.loc[:,~df.columns.duplicated()]\n",
    "\n",
    "        df.index = [race_id for i in range(len(df))]\n",
    "\n",
    "        if number_of_people == 7:\n",
    "            race_data_7 = pd.concat([race_data_7, df])\n",
    "        elif number_of_people == 9:\n",
    "            race_data_9 = pd.concat([race_data_9, df])\n",
    "        else:\n",
    "            continue\n",
    "    except:\n",
    "        print(url)\n",
    "        print(traceback.format_exc())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. データを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_7_y = race_data_7['result']\n",
    "data_7_x = race_data_7.drop('result', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.to_pickle(data_7_y, 'data/line_data_7y')\n",
    "pd.to_pickle(data_7_x, 'data/line_data_7x')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ea3eeac9e68bc0e3f703f37c7e2955c9932483a506e1740cfbadba66422ad64"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('keirin_prediction': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
